# Pytorch API

### 1. import torch

import & vision

~~~python
import torch 
print(torch.__version__)
~~~

### 2. Tensor type ğŸŒŸ

Pytorch ç»™å‡ºäº† 9 ç§ CPU Tensor ç±»å‹å’Œ 9 ç§ GPU Tensor ç±»å‹ã€‚Pytorch ä¸­é»˜è®¤çš„æ•°æ®ç±»å‹æ˜¯torch.FloatTensor, å³ torch.Tensor ç­‰åŒäº torch.FloatTensorã€‚

| Data type                | dtype                         | CPU tensor                            | GPU tensor              |
| ------------------------ | ----------------------------- | ------------------------------------- | ----------------------- |
| 32-bit floating point    | torch.float32 or torch.float  | torch.FloatTensor                     | torch.cuda.FloatTensor  |
| 64-bit floating point    | torch.float64 or torch.double | torch.DoubleTensor                    | torch.cuda.DoubleTensor |
| 16-bit floating point    | torch.float16 or torch.half   | torch.HalfTensor                      | torch.cuda.HalfTensor   |
| 8-bit integer (unsigned) | torch.uint8                   | torch.ByteTensor                      | torch.cuda.ByteTensor   |
| 8-bit integer (signed)   | torch.int8                    | torch.CharTensor                      | torch.cuda.CharTensor   |
| 16-bit integer (signed)  | torch.int16 or torch.short    | torch.ShortTensor                     | torch.cuda.ShortTensor  |
| 32-bit integer (signed)  | torch.int32 or torch.int      | torch.IntTensor                       | torch.cuda.IntTensor    |
| 64-bit integer (signed)  | torch.int64 or torch.long     | torch.LongTensor                      | torch.cuda.LongTensor   |
| Boolean                  | torch.bool                    | [torch.BoolTensor](#torch.BoolTensor) | torch.cuda.BoolTensor   |

##### è®¾ç½®é»˜è®¤Tensor ç±»å‹

Pytorch å¯ä»¥é€šè¿‡ `set_default_tensor_type` å‡½æ•°**è®¾ç½®é»˜è®¤ä½¿ç”¨çš„Tensorç±»å‹**ï¼Œ åœ¨å±€éƒ¨ä½¿ç”¨å®Œåå¦‚æœéœ€è¦å…¶ä»–ç±»å‹ï¼Œåˆ™è¿˜éœ€è¦é‡æ–°è®¾ç½®ä¼šæ‰€éœ€çš„ç±»å‹ 

~~~
torch.set_default_tensor_type('torch.DoubleTensor')
~~~

##### CPU/GPU äº’è½¬

CPU Tensor å’Œ GPU Tensor çš„åŒºåˆ«åœ¨äºï¼Œ å‰è€…å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼Œè€Œåè€…å­˜å‚¨åœ¨æ˜¾å­˜ä¸­ã€‚ä¸¤è€…ä¹‹é—´çš„è½¬æ¢å¯ä»¥é€šè¿‡ `.cpu()`ã€`.cuda()`å’Œ `.to(device)` æ¥å®Œæˆ  â€»

~~~python
>>> device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")  
>>> a = torch.rand(2,3)
>>> a = a.cuda() # CPU -> GPU
>>> a.type()
'torch.cuda.FloatTensor'
>>> a = a.cpu() # GPU -> CPU
>>> a.type()
'torch.FloatTensor'
>>> a = a.to(device) # CPU <->  GPU
>>> a.type()
'torch.cuda.FloatTensor'
~~~

##### åˆ¤å®š Tensor ç±»å‹çš„å‡ ç§æ–¹å¼: â€»

~~~python
>>> a
tensor([[0.6065, 0.0122, 0.4473],
        [0.5937, 0.5530, 0.4663]], device='cuda:0')
>>> a.is_cuda  # å¯ä»¥æ˜¾ç¤ºæ˜¯å¦åœ¨æ˜¾å­˜ä¸­
True
>>> a.dtype  # Tensor å†…éƒ¨dataçš„ç±»å‹
torch.float32
>>> a.type()
'torch.cuda.FloatTensor'  # å¯ä»¥ç›´æ¥æ˜¾ç¤ºTensorç±»å‹ = is_cuda + dtype
~~~

##### ç±»å‹è½¬æ¢

~~~python
>>> a
tensor([[0.6065, 0.0122, 0.4473],
        [0.5937, 0.5530, 0.4663]], device='cuda:0')
>>> a.type(torch.DoubleTensor)   # ä½¿ç”¨ dtype() å‡½æ•°è¿›è¡Œè½¬æ¢
tensor([[0.6065, 0.0122, 0.4473],
        [0.5937, 0.5530, 0.4663]], dtype=torch.float64)
>>> a = a.double()  # ç›´æ¥ä½¿ç”¨ int()ã€float() å’Œ double() ç­‰ç›´æ¥è¿›è¡Œæ•°æ®ç±»å‹è½¬æ¢è¿›è¡Œ
tensor([[0.6065, 0.0122, 0.4473],
        [0.5937, 0.5530, 0.4663]], device='cuda:0', dtype=torch.float64)
>>> b = torch.randn(4,5)
>>> b.type_as(a)  # ä½¿ç”¨ type_as å‡½æ•°å¹¶ä¸éœ€è¦æ˜ç¡®å…·ä½“æ˜¯å“ªç§ç±»å‹
tensor([[ 0.2129,  0.1877, -0.0626,  0.4607, -1.0375],
        [ 0.7222, -0.3502,  0.1288,  0.6786,  0.5062],
        [-0.4956, -0.0793,  0.7590, -1.0932, -0.1084],
        [-2.2198,  0.3827,  0.2735,  0.5642,  0.6771]], device='cuda:0',
       dtype=torch.float64)
~~~

##### Tensor ç›¸å…³ä¿¡æ¯è·å–

~~~python
torch.size()/torch.shape   # ä¸¤è€…ç­‰ä»·ï¼Œ è¿”å›tçš„å½¢çŠ¶, å¯ä»¥ä½¿ç”¨ x.size()[1] æˆ– x.size(1) æŸ¥çœ‹åˆ—æ•°
torch.numel() / torch.nelement()  # ä¸¤è€…ç­‰ä»·, tä¸­å…ƒç´ æ€»ä¸ªæ•°
a.item()  # å–å‡ºå•ä¸ªtensorçš„å€¼
~~~



### 3. Tensor Create

##### æœ€åŸºæœ¬çš„Tensoråˆ›å»ºæ–¹å¼

~~~python
troch.Tensor(2, 2) # ä¼šä½¿ç”¨é»˜è®¤çš„ç±»å‹åˆ›å»º Tensor, å¯ä»¥é€šè¿‡ torch.set_default_tensor_type('torch.DoubleTensor') è¿›è¡Œä¿®æ”¹
torch.DoubleTensor(2, 2) # æŒ‡å®šç±»å‹åˆ›å»º Tensor

torch.Tensor([[1, 2], [3, 4]])  # é€šè¿‡ list åˆ›å»º Tensor          å°† Tensorè½¬æ¢ä¸ºlistå¯ä»¥ä½¿ç”¨: t.tolist():
torch.from_numpy(np.array([2, 3.3]) ) # é€šè¿‡ numpy array åˆ›å»º tensor
~~~

##### ç¡®å®šåˆå§‹å€¼çš„æ–¹å¼åˆ›å»º

~~~python
ones(sizes)  # å…¨ 1 Tensor     
zeros(sizes)  # å…¨ 0 Tensor
eye(sizes)  # å¯¹è§’çº¿ä¸º1ï¼Œä¸è¦æ±‚è¡Œåˆ—ä¸€è‡´
full(sizes, value) # æŒ‡å®š value
~~~

##### åˆ†å¸ƒ

~~~python
rand(sizes)  # å‡åŒ€åˆ†å¸ƒ   
randn(sizes)   # æ ‡å‡†åˆ†å¸ƒ

# æ­£æ€åˆ†å¸ƒ: è¿”å›ä¸€ä¸ªå¼ é‡ï¼ŒåŒ…å«ä»ç»™å®šå‚æ•° means,std çš„ç¦»æ•£æ­£æ€åˆ†å¸ƒä¸­æŠ½å–éšæœºæ•°ã€‚ å‡å€¼ meansæ˜¯ä¸€ä¸ªå¼ é‡ï¼ŒåŒ…å«æ¯ä¸ªè¾“å‡ºå…ƒç´ ç›¸å…³çš„æ­£æ€åˆ†å¸ƒçš„å‡å€¼ -> ä»¥æ­¤å¼ é‡çš„å‡å€¼ä½œä¸ºå‡å€¼
# stdæ˜¯ä¸€ä¸ªå¼ é‡ï¼ŒåŒ…å«æ¯ä¸ªè¾“å‡ºå…ƒç´ ç›¸å…³çš„æ­£æ€åˆ†å¸ƒçš„æ ‡å‡†å·® -> ä»¥æ­¤å¼ é‡çš„æ ‡å‡†å·®ä½œä¸ºæ ‡å‡†å·®ã€‚ å‡å€¼å’Œæ ‡å‡†å·®çš„å½¢çŠ¶ä¸é¡»åŒ¹é…ï¼Œä½†æ¯ä¸ªå¼ é‡çš„å…ƒç´ ä¸ªæ•°é¡»ç›¸åŒ
torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))
tensor([-0.1987,  3.1957,  3.5459,  2.8150,  5.5398,  5.6116,  7.5512,  7.8650,
         9.3151, 10.1827])
uniform(from,to) # å‡åŒ€åˆ†å¸ƒ 

arange(s, e, steps)  # ä»såˆ°eï¼Œæ­¥é•¿ä¸ºstep
linspace(s, e, num)   # ä»såˆ°e,å‡åŒ€åˆ‡åˆ†ä¸º num ä»½, æ³¨æ„linespaceå’Œarangeçš„åŒºåˆ«ï¼Œå‰è€…çš„æœ€åä¸€ä¸ªå‚æ•°æ˜¯ç”Ÿæˆçš„Tensorä¸­å…ƒç´ çš„æ•°é‡ï¼Œè€Œåè€…çš„æœ€åä¸€ä¸ªå‚æ•°æ˜¯æ­¥é•¿ã€‚
randperm(m) # 0 åˆ° m-1 çš„éšæœºåºåˆ—
~~~

### 4. ç´¢å¼•ã€æ¯”è¾ƒã€æ’åº

##### ç´¢å¼•æ“ä½œ

~~~python
a[row, column]  # row è¡Œï¼Œ cloumn åˆ—
a[index]  # ç¬¬index è¡Œ
a[:,index]  # ç¬¬ index åˆ—

a[0, -1] #ç¬¬é›¶è¡Œï¼Œ æœ€åä¸€ä¸ªå…ƒç´ 
a[:index] # å‰ index è¡Œ
a[:row, 0:1] # å‰ row è¡Œï¼Œ 0å’Œ1åˆ—

a[a>1] # é€‰æ‹© a > 1çš„å…ƒç´ ï¼Œ ç­‰ä»·äº a.masked_select(a>1)
torch.nonzero(a) # é€‰æ‹©éé›¶å…ƒç´ çš„åæ ‡ï¼Œå¹¶è¿”å›
a.clamp(x, y) # å¯¹ Tensor å…ƒç´ è¿›è¡Œé™åˆ¶ï¼Œ å°äºxç”¨xä»£æ›¿ï¼Œ å¤§äºyç”¨yä»£æ›¿
torch.where(condition, x, y) # æ»¡è¶³condition çš„ä½ç½®è¾“å‡ºxï¼Œ å¦åˆ™è¾“å‡ºy
>>> a
tensor([[ 6., -2.],
        [ 8.,  0.]])
>>> torch.where(a>1, torch.full_like(a, 1), a) # å¤§äº1 çš„éƒ¨åˆ†ç›´æ¥ç”¨1ä»£æ›¿ï¼Œ å…¶ä»–ä¿ç•™åŸå€¼
tensor([[ 1., -2.],
        [ 1.,  0.]])
~~~

##### æ¯”è¾ƒæ“ä½œ

~~~python
gt >    lt <     ge >=     le <=   eq ==    ne != 
topk(input, k) -> (Tensor, LongTensor)
sort(input) -> (Tensor, LongTensor)
max/min => max(tensor)      max(tensor, dim)    max(tensor1, tensor2)
~~~

sort å‡½æ•°æ¥å—ä¸¤ä¸ªå‚æ•°, å…¶ä¸­ å‚æ•° 0 ä¸ºæŒ‰ç…§è¡Œæ’åºã€1ä¸ºæŒ‰ç…§åˆ—æ’åº: True ä¸ºé™åºï¼Œ False ä¸ºå‡åºï¼Œ è¿”å›å€¼æœ‰ä¸¤ä¸ªï¼Œ ç¬¬ä¸€ä¸ªæ˜¯æ’åºç»“æœï¼Œ ç¬¬äºŒä¸ªæ˜¯æ’åºåºå·

~~~python
>>> import torch
>>> a = torch.randn(3, 3)
>>> a
tensor([[-1.8500, -0.2005,  1.4475],
        [-1.7795, -0.4968, -1.8965],
        [ 0.5798, -0.1554,  1.6395]])
>>> a.sort(0, True)[0] 
tensor([[ 0.5798, -0.1554,  1.6395],
        [-1.7795, -0.2005,  1.4475],
        [-1.8500, -0.4968, -1.8965]])
>>> a.sort(0, True)[1]
tensor([[2, 2, 2],
        [1, 0, 0],
        [0, 1, 1]])
>>> a.sort(1, True)[1]
tensor([[2, 1, 0],
        [1, 0, 2],
        [2, 0, 1]])
>>> a.sort(1, True)[0]
tensor([[ 1.4475, -0.2005, -1.8500],
        [-0.4968, -1.7795, -1.8965],
        [ 1.6395,  0.5798, -0.1554]])
~~~



### 5. Element-wise å’Œ å½’å¹¶æ“ä½œ

Element-wise: è¾“å‡ºçš„ tensor å½¢çŠ¶ä¸åŸå§‹çš„å½¢çŠ¶ä¸€è‡´

~~~python
abs/sqrt/div/exp/fmod/log/pow...
cos/sin/asin/atan2/cosh...
ceil/round/floor/trunc
clamp(input, min, max)
sigmoid/tanh...
~~~

å½’å¹¶æ“ä½œï¼š è¾“å‡ºçš„tensorå½¢çŠ¶å°äºåŸå§‹çš„ Tensorå½¢çŠ¶

~~~python
mean/sum/median/mode   # å‡å€¼/å’Œ/ ä¸­ä½æ•°/ä¼—æ•°
norm/dist  # èŒƒæ•°/è·ç¦»
std/var  # æ ‡å‡†å·®/æ–¹å·®
cumsum/cumprd # ç´¯åŠ /ç´¯ä¹˜
~~~



### 6. å˜å½¢æ“ä½œ

##### view/resize/reshape **è°ƒæ•´Tensorçš„å½¢çŠ¶**

- å…ƒç´ æ€»æ•°å¿…é¡»ç›¸åŒ  
- view å’Œ reshape å¯ä»¥ä½¿ç”¨-1è‡ªåŠ¨è®¡ç®—ç»´åº¦
- å…±äº«å†…å­˜

! view() æ“ä½œæ˜¯éœ€è¦ Tensor åœ¨å†…å­˜ä¸­è¿ç»­çš„ï¼Œ è¿™ç§æƒ…å†µä¸‹éœ€è¦ä½¿ç”¨ contiguous() æ“ä½œå…ˆå°†å†…å­˜å˜ä¸ºè¿ç»­ã€‚ å¯¹äºreshape æ“ä½œï¼Œ å¯ä»¥çœ‹åšæ˜¯ `Tensor.contiguous().view()`.

~~~python
>>> a = torch.Tensor(2,2)
>>> a
tensor([[6.0000e+00, 8.0000e+00],
        [1.0000e+00, 1.8367e-40]])
>>> a.resize(4, 1)
tensor([[6.0000e+00],
        [8.0000e+00],
        [1.0000e+00],
        [1.8367e-40]])
~~~

##### transpose / permute  å„ç»´åº¦ä¹‹é—´çš„å˜æ¢ï¼Œ 

transpose å¯ä»¥å°†æŒ‡å®šçš„ä¸¤ä¸ªç»´åº¦çš„å…ƒç´ è¿›è¡Œè½¬ç½®ï¼Œ permute åˆ™å¯ä»¥æŒ‰ç…§æŒ‡å®šçš„ç»´åº¦è¿›è¡Œç»´åº¦å˜æ¢

~~~python
>>> x
tensor([[[-0.9699, -0.3375, -0.0178]],
        [[ 1.4260, -0.2305, -0.2883]]])

>>> x.shape
torch.Size([2, 1, 3])
>>> x.transpose(0, 1) # shape => torch.Size([1, 2, 3])
tensor([[[-0.9699, -0.3375, -0.0178],
         [ 1.4260, -0.2305, -0.2883]]])
>>> x.permute(1, 0, 2) # shape => torch.Size([1, 2, 3])
tensor([[[-0.9699, -0.3375, -0.0178],
         [ 1.4260, -0.2305, -0.2883]]])
>>> 
~~~

##### squeeze(dim)/unsquence(dim)  

å¤„ç†sizeä¸º1çš„ç»´åº¦ï¼Œ å‰è€…ç”¨äºå»é™¤sizeä¸º1çš„ç»´åº¦ï¼Œ è€Œåè€…åˆ™æ˜¯å°†æŒ‡å®šçš„ç»´åº¦çš„sizeå˜ä¸º1

~~~python
>>> a = torch.arange(1, 4)
>>> a
tensor([1, 2, 3]) # shape => torch.Size([3])
>>> a.unsqueeze(0) # shape => torch.Size([1, 3])
>>> a.unqueeze(0).squeeze(0) # shape => torch.Size([3])
~~~

##### expand/ expand_as/repeatå¤åˆ¶å…ƒç´ æ¥æ‰©å±•ç»´åº¦

æœ‰æ—¶éœ€è¦é‡‡ç”¨å¤åˆ¶çš„å½¢å¼æ¥æ‰©å±• Tensor çš„ç»´åº¦ï¼Œ è¿™æ—¶å¯ä»¥ä½¿ç”¨ expandï¼Œ expand() å‡½æ•°å°† size ä¸º 1çš„ç»´åº¦å¤åˆ¶æ‰©å±•ä¸ºæŒ‡å®šå¤§å°ï¼Œ ä¹Ÿå¯ä»¥ç”¨ expand_as() å‡½æ•°æŒ‡å®šä¸º ç¤ºä¾‹ Tensor çš„ç»´åº¦ã€‚

!! expand æ‰©å¤§ tensor ä¸éœ€è¦åˆ†é…æ–°å†…å­˜ï¼Œåªæ˜¯ä»…ä»…æ–°å»ºä¸€ä¸ª tensor çš„è§†å›¾ï¼Œå…¶ä¸­é€šè¿‡å°† stride è®¾ä¸º0ï¼Œä¸€ç»´å°†ä¼šæ‰©å±•ä½æ›´é«˜ç»´ã€‚

repeat æ²¿ç€æŒ‡å®šçš„ç»´åº¦é‡å¤ tensorã€‚ ä¸åŒäº expand()ï¼Œå¤åˆ¶çš„æ˜¯ tensor ä¸­çš„æ•°æ®ã€‚

~~~python
>>> a = torch.rand(2, 2, 1)
>>> a
tensor([[[0.3094],
         [0.4812]],

        [[0.0950],
         [0.8652]]])
>>> a.expand(2, 2, 3) # å°†ç¬¬2ç»´çš„ç»´åº¦ç”±1å˜ä¸º3ï¼Œ åˆ™å¤åˆ¶è¯¥ç»´çš„å…ƒç´ ï¼Œå¹¶æ‰©å±•ä¸º3
tensor([[[0.3094, 0.3094, 0.3094],
         [0.4812, 0.4812, 0.4812]],

        [[0.0950, 0.0950, 0.0950],
         [0.8652, 0.8652, 0.8652]]])

>>> a.repeat(1, 2, 1) # å°†ç¬¬äºŒä½å¤åˆ¶ä¸€æ¬¡
tensor([[[0.3094],
         [0.4812],
         [0.3094],
         [0.4812]],

        [[0.0950],
         [0.8652],
         [0.0950],
         [0.8652]]])
~~~



### 7. ç»„åˆä¸åˆ†å—

**ç»„åˆæ“ä½œ** æ˜¯å°†ä¸åŒçš„ Tensor å åŠ èµ·æ¥ã€‚ ä¸»è¦æœ‰ `cat()` å’Œ `torch.stack()` ä¸¤ä¸ªå‡½æ•°ï¼Œcat å³ concatenate çš„æ„æ€ï¼Œ æ˜¯æŒ‡æ²¿ç€å·²æœ‰çš„æ•°æ®çš„æŸä¸€ç»´åº¦è¿›è¡Œæ‹¼æ¥ï¼Œ æ“ä½œåçš„æ•°æ®çš„æ€»ç»´æ•°ä¸å˜ï¼Œ åœ¨è¿›è¡Œæ‹¼æ¥æ—¶ï¼Œ é™¤äº†æ‹¼æ¥çš„ç»´åº¦ä¹‹å¤–ï¼Œ å…¶ä»–ç»´åº¦å¿…é¡»ç›¸åŒã€‚ è€Œ` torch. stack()` å‡½æ•° æŒ‡æ–°å¢ç»´åº¦ï¼Œ å¹¶æŒ‰ç…§æŒ‡å®šçš„ç»´åº¦è¿›è¡Œå åŠ ã€‚

**åˆ†å—æ“ä½œ** æ˜¯æŒ‡å°† Tensor åˆ†å‰²æˆä¸åŒçš„å­Tensorï¼Œä¸»è¦æœ‰ `torch.chunk()` ä¸ `torch.split()` ä¸¤ä¸ªå‡½æ•°ï¼Œå‰è€…éœ€è¦æŒ‡å®šåˆ†å—çš„æ•°é‡ï¼Œè€Œåè€…åˆ™éœ€è¦æŒ‡å®šæ¯ä¸€å—çš„å¤§å°ï¼Œä»¥æ•´å½¢æˆ–è€…listæ¥è¡¨ç¤ºã€‚

~~~python
>>> a = torch.Tensor([[1,2,3], [4,5,6]])
>>> torch.chunk(a, 2, 0)
(tensor([[1., 2., 3.]]), tensor([[4., 5., 6.]]))
>>> torch.chunk(a, 2, 1)
(tensor([[1., 2.],
        [4., 5.]]), tensor([[3.],
        [6.]]))
>>> torch.split(a, 2, 0)
(tensor([[1., 2., 3.],
        [4., 5., 6.]]),)
>>> torch.split(a, [1, 2], 1)
(tensor([[1.],
        [4.]]), tensor([[2., 3.],
        [5., 6.]]))
~~~



### 8. linear algebra

~~~python
trace  # å¯¹è§’çº¿å…ƒç´ ä¹‹å’Œ(çŸ©é˜µçš„è¿¹)
diag  # å¯¹è§’çº¿å…ƒç´ 
triu/tril  # çŸ©é˜µçš„ä¸Šä¸‰è§’/ä¸‹ä¸‰è§’
mm/bmm   # çŸ©é˜µçš„ä¹˜æ³•ï¼Œ batchçš„çŸ©é˜µä¹˜æ³•
addmm/addbmm/addmv/addr/badbmm...  # çŸ©é˜µè¿ç®—
t # è½¬ç½®
dor/cross # å†…ç§¯/å¤–ç§¯
inverse # çŸ©é˜µæ±‚é€†
svd  # å¥‡å¼‚å€¼åˆ†è§£
~~~



### 9. åŸºæœ¬æœºåˆ¶

##### å¹¿æ’­æœºåˆ¶

ä¸åŒå½¢çŠ¶çš„Tensor è¿›è¡Œè®¡ç®—æ—¶ï¼Œ å¯ä»¥è‡ªåŠ¨æ‰©å±•åˆ°è¾ƒå¤§çš„ç›¸åŒå½¢çŠ¶å†è¿›è¡Œè®¡ç®—ã€‚ å¹¿æ’­æœºåˆ¶çš„å‰ææ˜¯ä¸€ä¸ªTensor è‡³å°‘æœ‰ä¸€ä¸ªç»´åº¦ï¼Œä¸”ä»å°¾éƒ¨éå†Tensoræ—¶ï¼Œä¸¤è€…ç»´åº¦å¿…é¡»ç›¸ç­‰ï¼Œ å…¶ä¸­ä¸ƒä¸ªè¦ä¹ˆæ˜¯1ï¼Œ è¦ä¹ˆä¸å­˜åœ¨

##### å‘é‡åŒ–æ“ä½œ

å¯ä»¥åœ¨åŒä¸€æ—¶é—´è¿›è¡Œæ‰¹é‡åœ°å¹¶è¡Œè®¡ç®—ï¼Œä¾‹å¦‚çŸ©é˜µè¿ç®—ï¼Œä»¥è¾¾åˆ°æ›´é«˜çš„è®¡ç®—æ•ˆç‡çš„ä¸€ç§æ–¹å¼:

##### å…±äº«å†…å­˜æœºåˆ¶

(1) ç›´æ¥é€šè¿‡ Tensor æ¥åˆå§‹åŒ–å¦ä¸€ä¸ª Tensorï¼Œ æˆ–è€…é€šè¿‡ Tensor çš„ç»„åˆã€åˆ†å—ã€ç´¢å¼•ã€å˜å½¢æ¥åˆå§‹åŒ–å¦ä¸€ä¸ªTensorï¼Œ åˆ™è¿™ä¸¤ä¸ªTensor å…±äº«å†…å­˜:

~~~python
>>> a = torch.randn(2,3)
>>> b = a
>>> c = a.view(6)
>>> b[0, 0] = 0
>>> c[3] = 4
>>> a
tensor([[ 0.0000,  0.3898, -0.7641],
        [ 4.0000,  0.6859, -1.5179]])
~~~

(2) å¯¹äºä¸€äº›æ“ä½œé€šè¿‡åŠ å‰ç¼€  â€œ\_â€  å®ç° inplace æ“ä½œï¼Œ å¦‚ `add_()` å’Œ `resize_()` ç­‰ï¼Œ è¿™æ ·æ“ä½œåªè¦è¢«æ‰§è¡Œï¼Œ æœ¬èº«çš„ Tensor å°±ä¼šè¢«æ”¹å˜ã€‚

~~~
>>> a
tensor([[ 0.0000,  0.3898, -0.7641],
        [ 4.0000,  0.6859, -1.5179]])
>>> a.add_(a)
tensor([[ 0.0000,  0.7796, -1.5283],
        [ 8.0000,  1.3719, -3.0358]])
~~~

(3) Tensorä¸ Numpy å¯ä»¥é«˜æ•ˆçš„å®Œæˆè½¬æ¢ï¼Œ å¹¶ä¸”è½¬æ¢å‰åçš„å˜é‡å…±äº«å†…å­˜ã€‚åœ¨è¿›è¡Œ Pytorch ä¸æ”¯æŒçš„æ“ä½œçš„æ—¶å€™ï¼Œ ç”šè‡³å¯ä»¥æ›²çº¿æ•‘å›½ï¼Œ å°† Tensor è½¬æ¢ä¸º Numpy ç±»å‹ï¼Œæ“ä½œåå†è½¬åŒ–ä¸º Tensor

~~~
# tensor <--> numpy
b = a.numpy() # tensor -> numpy
a = torch.from_numpy(a) # numpy -> tensor
~~~

!! éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ**torch.tensor() æ€»æ˜¯ä¼šè¿›è¡Œæ•°æ®æ‹·è´ï¼Œæ–°tensorå’ŒåŸæ¥çš„æ•°æ®ä¸å†å…±äº«å†…å­˜**ã€‚æ‰€ä»¥å¦‚æœä½ æƒ³å…±äº«å†…å­˜çš„è¯ï¼Œå»ºè®®ä½¿ç”¨ `torch.from_numpy()` æˆ–è€… `tensor.detach()` æ¥æ–°å»ºä¸€ä¸ªtensor, äºŒè€…å…±äº«å†…å­˜ã€‚



### 10. nn

~~~python
from torch import nn
~~~

##### pad å¡«å……

~~~python
nn.ConstantPad2d(padding, value)
~~~

##### å·ç§¯å’Œåå·ç§¯

~~~python
nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)
nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, groups=1, bias=True, dilation=1)
~~~

##### æ± åŒ–å±‚

~~~python
nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)
nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True)
nn.AdaptiveMaxPool2d(output_size, return_indices=False)
nn.AdaptiveAvgPool2d(output_size)
nn.MaxUnpool2d(kernel_size, stride=None, padding=0)
~~~

##### å…¨è¿æ¥å±‚

~~~python
nn.Linear(in_features, out_features, bias=True)
~~~

##### é˜²æ­¢è¿‡æ‹Ÿåˆç›¸å…³å±‚

~~~python
nn.Dropout2d(p=0.5, inplace=False)
nn.AlphaDropout(p=0.5)
nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
~~~

##### æ¿€æ´»å‡½æ•°

~~~python
nn.Softplus(beta=1, threshold=20)
nn.Tanh()
nn.ReLU(inplace=False)    
nn.ReLU6(inplace=False)
nn.LeakyReLU(negative_slope=0.01, inplace=False)
nn.PReLU(num_parameters=1, init=0.25)
nn.SELU(inplace=False)
nn.ELU(alpha=1.0, inplace=False)
~~~

##### RNN 

~~~python
nn.RNNCell(input_size, hidden_size, bias=True, nonlinearity='tanh')
nn.RNN(*args, **kwargs)
nn.LSTMCell(input_size, hidden_size, bias=True)
nn.LSTM(*args, **kwargs)
nn.GRUCell(input_size, hidden_size, bias=True)
nn.GRU(*args, **kwargs)
~~~

##### Embedding

~~~python
nn.Embedding(num_embeddings, embedding_dim, padding_idx=None, max_norm=None, norm_type=2, scale_grad_by_freq=False, sparse=False, _weight=None)
~~~

##### Sequential

~~~python
nn.Sequential(*args)
~~~

##### loss functon

~~~python
nn.BCELoss(weight=None, size_average=True, reduce=True)
nn.CrossEntropyLoss(weight=None, size_average=True, ignore_index=-100, reduce=True)
nn.L1Loss(size_average=True, reduce=True)
nn.KLDivLoss(size_average=True, reduce=True)
nn.MSELoss(size_average=True, reduce=True)
nn.NLLLoss(weight=None, size_average=True, ignore_index=-100, reduce=True)
nn.NLLLoss2d(weight=None, size_average=True, ignore_index=-100, reduce=True)
nn.SmoothL1Loss(size_average=True, reduce=True)
nn.SoftMarginLoss(size_average=True, reduce=True)
nn.TripletMarginLoss(margin=1.0, p=2, eps=1e-06, swap=False, size_average=True, reduce=True)
nn.CosineEmbeddingLoss(margin=0, size_average=True, reduce=True)
~~~

##### functional

~~~python
nn.functional # nnä¸­çš„å¤§å¤šæ•°layerï¼Œåœ¨functionalä¸­éƒ½æœ‰ä¸€ä¸ªä¸ä¹‹ç›¸å¯¹åº”çš„å‡½æ•°ã€‚
              # nn.functionalä¸­çš„å‡½æ•°å’Œnn.Moduleçš„ä¸»è¦åŒºåˆ«åœ¨äºï¼Œ
              # ç”¨nn.Moduleå®ç°çš„layersæ˜¯ä¸€ä¸ªç‰¹æ®Šçš„ç±»ï¼Œéƒ½æ˜¯ç”± class layer(nn.Module)å®šä¹‰ï¼Œ
              # ä¼šè‡ªåŠ¨æå–å¯å­¦ä¹ çš„å‚æ•°ã€‚è€Œnn.functionalä¸­çš„å‡½æ•°æ›´åƒæ˜¯çº¯å‡½æ•°ï¼Œ
              # ç”±def function(input)å®šä¹‰ã€‚
~~~

##### init

~~~python
torch.nn.init.uniform
torch.nn.init.normal
torch.nn.init.kaiming_uniform
torch.nn.init.kaiming_normal
torch.nn.init.xavier_normal
torch.nn.init.xavier_uniform
torch.nn.init.sparse
~~~

##### net

~~~python
class net_name(nn.Module):
    def __init__(self):
        super(net_name, self).__init__()
        self.layer_name = xxxx

    def forward(self, x): 
        x = self.layer_name(x)        
        return x

net.parameters()   # è·å–å‚æ•° 
net.named_parameters  # è·å–å‚æ•°åŠåç§°
net.zero_grad()  # ç½‘ç»œæ‰€æœ‰æ¢¯åº¦æ¸…é›¶, grad åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­æ˜¯ç´¯åŠ çš„(accumulated)ï¼Œè¿™æ„å‘³ç€æ¯ä¸€æ¬¡è¿è¡Œåå‘ä¼ æ’­ï¼Œæ¢¯åº¦éƒ½ä¼šç´¯åŠ ä¹‹å‰çš„æ¢¯åº¦ï¼Œæ‰€ä»¥åå‘ä¼ æ’­ä¹‹å‰éœ€æŠŠæ¢¯åº¦æ¸…é›¶ã€‚
~~~



### 11. optim -> form torch import optim

~~~python
optim.SGD(params, lr=0.01, momentum=0, dampening=0, weight_decay=0, nesterov=False)
optim.ASGD(params, lr=0.01, lambd=0.0001, alpha=0.75, t0=1000000.0, weight_decay=0)
optim.LBFGS(params, lr=1, max_iter=20, max_eval=None, tolerance_grad=1e-05, tolerance_change=1e-09, history_size=100, line_search_fn=None)
optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)
optim.Rprop(params, lr=0.01, etas=(0.5, 1.2), step_sizes=(1e-06, 50))
optim.Adadelta(params, lr=1.0, rho=0.9, eps=1e-06, weight_decay=0)
optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0)
optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)
optim.Adamax(params, lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)
optim.SparseAdam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08)
optim.Optimizer(params, defaults)

optimizer.zero_grad()  # ç­‰ä»·äº net.zero_grad() 
optimizer.step()
~~~



### 12. save and load model

~~~python
torch.save(model.state_dict(), 'xxxx_params.pth')
model.load_state_dict(t.load('xxxx_params.pth'))
torch.save(model, 'xxxx.pth')
model.torch.load('xxxx.pth')
all_data = dict(
    optimizer = optimizer.state_dict(),
    model = model.state_dict(),
    info = u'model and optim parameter'
)
t.save(all_data, 'xxx.pth')
all_data = t.load('xxx.pth')
all_data.keys()
~~~



### 13. torchvision

##### models

~~~python
from torchvision import models
resnet34 = models.resnet34(pretrained=True, num_classes=1000)
~~~

##### data augmentation  

~~~python
from torchvision import transforms

# transforms.CenterCrop           transforms.Grayscale           transforms.ColorJitter          
# transforms.Lambda               transforms.Compose             transforms.LinearTransformation 
# transforms.FiveCrop             transforms.Normalize           transforms.functional           
# transforms.Pad                  transforms.RandomAffine        transforms.RandomHorizontalFlip  
# transforms.RandomApply          transforms.RandomOrder         transforms.RandomChoice         
# transforms.RandomResizedCrop    transforms.RandomCrop          transforms.RandomRotation        
# transforms.RandomGrayscale      transforms.RandomSizedCrop     transforms.RandomVerticalFlip   
# transforms.ToTensor             transforms.Resize              transforms.transforms                                           
# transforms.TenCrop              transforms.Scale               transforms.ToPILImage
~~~

##### è‡ªå®šä¹‰ dataset

~~~python
from torch.utils.data import Dataset
class my_data(Dataset):
    def __init__(self, image_path, annotation_path, transform=None):
        # åˆå§‹åŒ–ï¼Œ è¯»å–æ•°æ®é›†
    def __len__(self):
        # è·å–æ•°æ®é›†çš„æ€»å¤§å°
    def __getitem__(self, id):
        # å¯¹äºåˆ¶å®šçš„ id, è¯»å–è¯¥æ•°æ®å¹¶è¿”å›    
~~~

##### datasets

**datasets**

~~~python
transform = transforms.Compose([
        transforms.ToTensor(), # vonvert to Tensor
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),]) # normalization
dataset = ImageFolder(root, transform=transform, target_transform=None, loader=default_loader)
dataloader = DataLoader(dataset, 2, collate_fn=my_collate_fn, num_workers=1,shuffle=True)
for batch_datas, batch_labels in dataloader:
    ...
~~~

##### img process

~~~python
img = make_grid(next(dataiter)[0], 4) 
save_image(img, 'a.png')
~~~

##### data Visualization

~~~python
from torchvision.transforms import ToPILImage
show = ToPILImage()     # å¯ä»¥æŠŠTensorè½¬æˆImageï¼Œæ–¹ä¾¿å¯è§†åŒ–

(data, label) = trainset[100]
show((data + 1) / 2).resize((100, 100))  # åº”è¯¥ä¼šè‡ªåŠ¨ä¹˜ä»¥ 255 çš„
~~~



### 14. Code Samples

~~~python
# torch.device object used throughout this script
device = torch.device("cuda" if use_cuda else "cpu")

model = MyRNN().to(device)

# train
total_loss = 0
for input, target in train_loader:
    input, target = input.to(device), target.to(device)
    hidden = input.new_zeros(*h_shape)  # has the same device & dtype as `input`
    ...  # get loss and optimize
    total_loss += loss.item()           # get Python number from 1-element Tensor

# evaluate
with torch.no_grad():                   # operations inside don't track history
    for input, target in test_loader:
        ...
~~~

